# README — Assignment: Heart Disease Classification

This README follows the required submission structure and is PDF-ready.

## a. Problem statement

Predict whether a patient has heart disease (binary classification) using clinical features from the UCI Heart Disease dataset. The aim is to compare multiple ML models and select the best-performing model based on accuracy, AUC, precision, recall, F1 and MCC.

## b. Dataset description [1 mark]

- Dataset: UCI Heart Disease (Kaggle)
- Samples (after preprocessing): 920
- Original features: 16 (final encoded features: 19)
- Target: `target` (0 = no disease, 1 = disease)
- Class counts: 0 → 411 (44.7%), 1 → 509 (55.3%)

Preprocessing summary:
- Converted `num` → binary `target`
- Dropped non-predictive columns (`id`, `dataset`)
- One-hot encoded categorical features (`sex`, `cp`, `restecg`, `slope`, `thal`)
- Filled numeric NaNs with median and categorical NaNs with mode
- StandardScaler applied for models that require scaling

## c. Models used (6 marks — 1 mark per model)

The following models were implemented and evaluated. Each model's evaluation metrics (Accuracy, AUC, Precision, Recall, F1, MCC) are shown in the comparison table below.

Comparison table — evaluation metrics for all 6 models:

| ML Model Name | Accuracy | AUC | Precision | Recall | F1 | MCC |
|---|---:|---:|---:|---:|---:|---:|
| Logistic Regression | 0.8370 | 0.9029 | 0.8333 | 0.8824 | 0.8571 | 0.6691 |
| Decision Tree | 0.7880 | 0.7813 | 0.7890 | 0.8431 | 0.8152 | 0.5691 |
| KNN | 0.8315 | 0.9020 | 0.8142 | 0.9020 | 0.8558 | 0.6594 |
| Naive Bayes | 0.8370 | 0.8899 | 0.8529 | 0.8529 | 0.8529 | 0.6700 |
| Random Forest (Ensemble) | **0.8533** | **0.9075** | **0.8505** | 0.8922 | **0.8708** | **0.7023** |
| XGBoost (Ensemble) | 0.8370 | 0.8892 | 0.8273 | **0.8922** | 0.8585 | 0.6695 |

### Notes on metrics
- Accuracy: proportion correct
- AUC: discrimination ability (higher better)
- Precision: TP/(TP+FP)
- Recall: TP/(TP+FN)
- F1: harmonic mean of precision & recall
- MCC: balanced correlation metric for binary classification

## Observations on model performance (3 marks)

| ML Model Name | Observation about model performance |
|---|---|
| Logistic Regression | Good baseline performance (Acc 0.8370, AUC 0.9029). Balanced precision and recall; interpretable but may miss complex non-linear interactions. |
| Decision Tree | Lowest overall accuracy (0.7880) and AUC (0.7813). Prone to overfitting; high variance; ensemble methods improve this. |
| KNN | Strong recall (0.9020), good AUC (0.9020), accuracy 0.8315. Sensitive to scaling; useful when prioritizing sensitivity. |
| Naive Bayes | Good precision (0.8529) and balanced metrics overall (Acc 0.8370). Fast and simple; independence assumption may be violated. |
| Random Forest (Ensemble) | Best overall performer (Acc 0.8533, AUC 0.9075, F1 0.8708, MCC 0.7023). Robust, handles non-linearities and interactions well. |
| XGBoost (Ensemble) | Competitive with Random Forest (Acc 0.8370, Recall 0.8922). Slightly lower AUC; requires careful hyperparameter tuning but offers strong recall. |

---

All content above is part of the required README and should also be included in the submitted PDF.

## Repository structure and Streamlit app

Repository layout (files included):

```
project-folder/
├── app.py               # Streamlit app (this file)
├── save_models.py       # Script to train and save model artifacts to model/
├── model/               # Saved model files and scaler (generated by save_models.py)
│   ├── Logistic_Regression.pkl
│   ├── Decision_Tree.pkl
│   ├── KNN.pkl
│   ├── Naive_Bayes.pkl
│   ├── Random_Forest.pkl
│   ├── XGBoost.pkl
│   ├── scaler.pkl
│   └── metrics_summary.csv
├── heart_disease_classifier.py  # Training pipeline and helper functions
├── heart_disease_uci.csv        # Dataset
├── requirements.txt
└── README.md
```

How to generate saved models (locally):

```bash
python save_models.py
```

How to run the Streamlit app locally:

```bash
pip install -r requirements.txt
streamlit run app.py
```

To deploy: push the repository to GitHub, then create a new app on Streamlit Cloud and point it to `app.py` on the `main` branch.

---

This completes the repository layout and Streamlit instructions.
